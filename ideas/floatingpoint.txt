Thoughts on modeling floating point operations

When I decide to tackle the F and D extensions, there are a few decisions that I
will have to make about how I want to do it.

* Integer arithmetic (status quo)

For integer arithmetic, I am modeling the behavior of the instructions using an
expression language, BVExpr, that builds expressions involving bit vectors,
without actually computing anything (because the input values are unknown). The
widths of the expressions are statically tracked.

In simulation, I define a straightforward interpretation of these expressions in
an environment where the value of the inputs *are* known, because we are
simulating against an actual machine state, and so we have values for the
operands, registers, and memory locations. The BVExprs are recursively
transformed into BitVectors.

* Semantics for floating point

The semantics for how BitVectors are manipulated as representing floating-point
data are complex. The core decision I will make is: do I want to extend my
BVExpr language to represent the various floating point operations? Or, do I
want to hard-code how those operations work using the existing constructors? If
I choose the former, that would have the benefit of keeping the semantics
simple, and would enable faster simulation. However, the semantics would have
less information baked in for how those floating point operations work. Instead
of knowng exactly how the significand, exponent, and sign bits are affected, all
we would have would be an expression like FPAdd x y, where x and y were
BitVectors. This would push the burden of interpreting the correctness of such
expressions to the user.

I am leaning toward this approach anyway. I do want simulation to remain
relatively fast, and I don't want my floating point semantics to be burdened by
low-level descriptions of how the various chunks of BitVectors are
manipulated. Furthermore, getting the semantics right for arbtrary BitVector
widths is going to be really tricky. So I think the correct approach is to push
the burden of interpreting the floating point operations onto the simulation and
verification environments, and leaving the semantics clean, uninterpreted, and
RISC-V focused.

* External libraries

For simulation, one idea I had would be to do some FFI calls to an external C
library like SoftFloat. I could support this natively in the bv-sized package, I
believe. I could also actually model the entire IEEE spec in native Haskell,
which would certainly be slower but would probably be OK for simulation, and
would have the benefit of forcing me to re-learn the low-level details of all
the corner cases. On the other hand, this would be a lot of effort when
SoftFloat has already done this for me. SoftFloat actually has native support
for 128-bit floats, which is a HUGE plus.
